%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

% Document info {{{

\def\documenttitle{Umělé neuronové sítě}
\def\documentdate{\today}
\def\documentauthor{Tomáš Vyčítal}
\def\documentkeywords{}
\def\languages{american,czech}

% }}}
% Shared configuration {{{

\documentclass[\languages,a4paper,12pt]{article}

% Design
\raggedright{}
\usepackage{parskip}

% Fonts
\usepackage[no-math]{fontspec} % no-math required by mathastext
\usepackage[light,semibold,default]{sourceserifpro}
\usepackage[light,semibold]{sourcesanspro}
\usepackage[light,semibold]{sourcecodepro}

% Other
\usepackage{babel}
\usepackage[autostyle]{csquotes}
\usepackage{titlesec}
\usepackage{siunitx}

% Math
\usepackage{mathtools}
\usepackage[italic]{mathastext}

% Spacing
\usepackage[onehalfspacing]{setspace}
\usepackage{enumitem}
\setlist{nosep}
\frenchspacing

% Unumbered but included in TOC
\newcommand{\unnumberedsection}[1]{%
	\section*{#1}
	\phantomsection\addcontentsline{toc}{section}{#1}
}

% Links in PDF
\usepackage{hyperref}
\hypersetup{
	allcolors=[RGB]{0,150,136},
	bookmarksnumbered=true,
	bookmarksopen=true,
	bookmarksopenlevel=2,
	colorlinks=true,
	unicode,
}
% has to be imported last to work properly

% Metadata
\title{\documenttitle}
\date{\documentdate}
\author{\documentauthor}
\pdfstringdefDisableCommands{\def\and{and }}
\hypersetup{
	pdftitle={\documenttitle},
	pdfauthor={\documentauthor},
	pdfkeywords={\documentkeywords},
}

% }}}

\begin{document}

\maketitle

\section{Abstrakt}

Cílem práce bylo vytvořit a vytrénovat neuronovou síť pro klasifikaci zvuků srdce do \num{5} kategorií (artifact, extrahls, extrastole, murmur, normal).
Pro klasifikaci byly použity normalizované PCM vzorky zvuků srdce a konvoluční neuronová síť.
Ve výsledku bylo dosaženo úspěšnosti klasifikace \SI{77}{\percent}, což je lepší než náhoda, nicméně nejedná se o nijak vynikající výsledek.
Jako hlavní bariéra pro dosažení lepších výsledků se s největší pravděpodobností jeví kvalita a množství dostupných dat.

\section{Data}

Pro práci byly zváženy dva různý zdroje dat.

\subsection{Předpřipravený dataset}\label{puvodni-dataset}

Původním záměrem bylo využít již připravený a předzpracovaný dataset\footnote{\url{http://www.timeseriesclassification.com/description.php?Dataset=AbnormalHeartbeat}} z veřejně dostupný kolekce Time Series Classification Website.
Nicméně při použití dat z tohoto datasetu se mi nepodařilo dosáhnout lepšího výsledku než \SI{60}{\percent} úspěšně klasifikovaných vzorků.
Tohoto výsledku se ale dá dosáhnout pomocí NN z 5 neuronů bez jakéhokoli připojení na vstupy (více v kapitole \nameref{vyvazenost-dat}).

Podle popisu byly data předpřipravený na \SI{4000}{\Hz} a zkrácený na délku nejkratšího vzorku.
Pokud by to ale byla pravda (pravděpodobně je, ale je obtížný to vyhodnotit s méně než \SI{1}{\s} audia ve velice nízký kvalitě), tak by každý vzorek byl zkrácen na délku necelých \SI{764}{\ms} a tím pádem zcela znehodnocen.

\subsection{Wav soubory}

Kvůli problémům, zmíněným v kapitole \nameref{puvodni-dataset}, byly pro další práci využity data přímo ze zdrojových Wav souborů\footnote{\url{https://www.kaggle.com/kinguistics/heartbeat-sounds}}.
V datech jsou k dispozici jednak zvukový soubory ve formátu Wav a kategorie pro jednotlivé záznamy.
Dále jsou pro některý vzorky jsou k dispozici i další údaje, jako například časový pozice jednotlivých úkazů v rámci vzorku, ale protože nejsou k dispozici pro všechny vzorky, tak nebyly nijak využity.

\begin{table}[htbp]
\centering
\begin{tabular}{ l r }
	kategorie  & počet vzorků \\
	\hline
	artifact   & \num{    40} \\
	extrahls   & \num{    19} \\
	extrastole & \num{    46} \\
	murmur     & \num{   129} \\
	normal     & \num{   351} \\
	\hline
	celkem     & \num{   585} \\
\end{tabular}
\caption{Dostupný data}
\end{table}

\subsection{Předzpracování}

Délka jednotlivých vzorků byla sjednocena na \SI{1}{\minute} a to buď zkrácením, pokud byla délka vzorku příliš dlouhá, nebo opakováním, pokud byla délka vzorku příliš krátká.
S kratší délkou vzorku se neuronová síť nebyla schopná téměř nic naučit, delší délka se pak ani s jednoduchou neuronovou sítí nevešla do paměti RAM, kterou mám k dispozici.

Byla zvážena i možnost dalšího zpracování, jako například FFT, nicméně hned při prvním pokusu snaha o trénování skončila vyčerpáním dostupný paměti RAM a pádem programu.
Při snížení délky vzorků a snížení počtu frekvencí tak, aby se nevyčerpala veškerá dostupná paměť, neuronová síť nevykazovala nijak lepší výsledky než při použítí PCM.
Značným problémem je tu vysoká výpočetní náročnost pro daný množství a délku vzorků, která velmi kompikuje práci, zejména při neznalosti ideálních parametrů pro danou úlohu.
Proto tato cesta nebyla dále prozkoumána.

\subsection{Vyváženost dat}\label{vyvazenost-dat}

Data jsou značně nevyvážená.
To vede k zavádějícím situacím, kdy lze dosáhnout úspěšnosti klasifikace přes \SI{60}{\percent}, aniž by bylo potřeba jakkoli vyhodnocovat vzorky.
Ke klasifikaci je zde \num{5} kategorií, takže by člověk v takovém případě instinktivně očekával cca. \SI{20}{\percent}.
Jedna z kategorií ale obsahuje \SI{60}{\percent} všech vzorků.
Na druhé straně jiná z kategorií obsahuje pouze \SI{3}{\percent} vzorků.

Z tohoto důvodu je potřeba vhodně upravit váhu jednotlivých kategorií pro co nejlepší výsledek klasifikace.
V opačném případě má neuronová síť dost silnou tendenci naprosto ignorovat nejmenší kategorie (klidně i všechny \num{4}).

\section{Architektura neuronový sítě}

Jedná se o sekvenční model s celkem \num{18} vrstvama.

V první části byla použita konvoluce s velikostí kernelů \num{19} a filtrů \num{16}.
Větší velikost kernelů se v tomto případě jeví značně výhodná, jelikož neuronová síť má velký problém se cokoli naučit s malou velikostí kernelů.
Jako aktivační funkce byla zvolena leaky ReLU.
Pro zabránění přeučení byla v této části použita batch normalizace.
Konvolučních vrstev je celkem \num{10} a jsou vždy ve skupinách po \num{2}, zakončených vrstvou max pool s velikostí poolu \num{3}.
Celá část je zakončena vrstvou global max pool.

Za konvoluční část byly umístěny \num{2} dense vrstvy, každá s \num{64} neurony.
Rovněž jako u konvoluce, byla i zde zvolena aktivační funce leaky ReLU.
Pro zabránění přeučení pak byly použity dropouty s \SI{50}{\percent} retencí.

Poslední vrstva má \num{5} neuronů a používá softmax aktivační funkci pro přiřazení výsledné pravděpodobnosti jednotlivým kategoriím.

\section{Výsledek}

Neuronová síť se dokázala naučit správně vyhodnotit \SI{77}{\percent} vzorků z kontrolní množiny (která nebyla použita pro trénování).
Výsledek by se určitě dal zlepšit další prací na designu neuronový sítě, ale osobně jako hlavní problém pro dosažení lepšího výsledku vidim nedostatek dat v některých kategoriích.
Při méně úspěšných pokusech se totiž docela často stávalo, že se neuronová síť naučila rozpoznávat ty největší kategorie, ale už ne ty menší, případně jen s mizivou úspěšností.
Podle mě by i stávající implementace dosáhla podstatně lepšího výsledku, pokud by každá kategorie obsahovala \num{117} vzorků (tj. stejně vzorků celkem, ale vyvážený dataset) nebo ideálně samozřejmně ještě více.

\begin{table}[htbp]
\centering
\begin{tabular}{ l r r r }
	kategorie   &    správně  &     celkem  &             správně  \\
	\hline
	artifact    &  \num{  9}  &  \num{ 10}  &  \SI{ 90}{\percent}  \\
	extrahls    &  \num{  4}  &  \num{  4}  &  \SI{100}{\percent}  \\
	extrastole  &  \num{  3}  &  \num{ 11}  &  \SI{ 27}{\percent}  \\
	murmur      &  \num{ 24}  &  \num{ 32}  &  \SI{ 75}{\percent}  \\
	normal      &  \num{ 71}  &  \num{ 87}  &  \SI{ 82}{\percent}  \\
	\hline
	celkem      &  \num{111}  &  \num{144}  &  \SI{ 77}{\percent}  \\
\end{tabular}
\caption{Klasifikace validačních dat}
\end{table}

\end{document}

% vim:sw=8:ts=8:fdm=marker
